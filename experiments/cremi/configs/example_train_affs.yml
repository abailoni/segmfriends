# Use "KeyDeleter" string if you want to delete a parameter that was set from an inherited config.


shortcuts:
  z_size: &z_size 12
  xy_size: &xy_size 272
  xy_size_precrop: &xy_size_precrop 302
  stride: &stride [10, 180, 180]
  padding: &dataset_padding [[0,0], [50,50], [50,50]]

device: cuda

loaders:
  general:
    volume_config:
      rejection_threshold: 0.20
      segmentation:
        affinity_config:
          retain_mask: False # This keep a mask of the valid affinities (not involving the ignore-label)
          retain_segmentation: True # This keeps the label image in the inputs
          ignore_label: 0

    defect_augmentation_config:
      keep_track_of:
        # - "artifacts"
        - "missing_slice"
      min_distance_between_defects: 2
      nb_contiguous_artifacts: 2
      # TODO: find defected slices in new data
#      ignore_slice_list:
#        B:
#          - 23
#          - 24
#          - 52
#          - 53
#        C:
#          - 22
#          - 82
#          - 94
      p_missing_slice: 0.006 #0.006
      p_low_contrast: 0.000
      p_deformed_slice: 0.000
      p_artifact_source: 0.003 # 0.006
      deformation_mode: 'compress'
      deformation_strength: 16
      artifact_source:
        min_masking_ratio: .5
        slicing_config:
          window_size:
            - 1
            - *xy_size_precrop
            - *xy_size_precrop
          stride: [1, 300, 300]
          downsampling_ratio: [1, 1, 1]
        volume_config:
          artifacts:
            path: '$HCI_HOME/datasets/cremi/constantin_data/sample_ABC_padded_20160501.defects.hdf'
            path_in_h5_dataset: 'defect_sections/raw_2x'
            dtype: float32
          alpha_mask:
            path: '$HCI_HOME/datasets/cremi/constantin_data/sample_ABC_padded_20160501.defects.hdf'
            path_in_h5_dataset: 'defect_sections/mask_2x'
        master_config:
          elastic_transform:
            alpha: 2000.
            sigma: 50.

    # Configuration for the master dataset.
    master_config:
      # We might need order 0 interpolation if we have segmentation in there somewhere.
#      crop_after_target:
#        - [0, 10, 10]
#        - [0, 10, 10]
      elastic_transform:
        apply: False
        alpha: 2000.
        sigma: 50.
        order: 0
      random_slides:
        shape_after_slide:
          - *xy_size
          - *xy_size
        shift_vs_slide_proba: 0.
        apply_proba: 0.5 # 0.2
        apply_to: [0] # Here we make sure that shifting (only one slice) does not change the GT

      random_flip: True
      defects_label: 3
      ignore_label: 0

      affinity_config:
        0:
          retain_mask: True # This keep a mask of the valid affinities (not involving the ignore-label)
          retain_segmentation: True # This keeps the label image as the first channel in the target tensor
          ignore_label: 0
          offsets:
            # Short-ranges:
            - [-1, 0, 0]
            - [0, -1, 0]
            - [0, 0, -1]
            # Mid-ranges:
            - [-1, -1, -1]
            - [-1, 1, 1]
            - [-1, -1, 1]
            - [-1, 1, -1]
            - [0, -9, 0]
            - [0, 0, -9]
            - [0, -9, -9]
            - [0, 9, -9]
            - [0, -9, -4]
            - [0, -4, -9]
            - [0, 4, -9]
            - [0, 9, -4]
            # Long-ranges:
            - [0, -27, 0]
            - [0, 0, -27]
            - [-2, 0, 0]
            - [-3, 0, 0]
            - [-4, 0, 0]

#      downscale_and_crop:
#        # Inputs:
#        - {ds_factor: [1, 1, 1],
#          crop_factor: [1, 1, 1],
#          apply_to: 0}
##        - {ds_factor: [1, 2, 2],
##          crop_factor: [1, 2, 2],
##          apply_to: 0}
#        # Targets:
#        - {ds_factor: [1, 1, 1],
#          crop_factor: [1, 1, 1],
#          apply_to: 1}
#        - {ds_factor: [1, 2, 2],
#          crop_factor: [1, 1, 1],
#          apply_to: 1}
#        - {ds_factor: [1, 4, 4],
#          crop_factor: [1, 1, 1],
#          apply_to: 1}


    # Specify configuration for the loader
    loader_config:
      # Number of processes to use for loading data. Set to (say) 10 if you wish to
      # use 10 CPU cores, or to 0 if you wish to use the same process for training and
      # data-loading (generally not recommended).
      batch_size: 1
      num_workers: 6
      drop_last: True
      pin_memory: False
      shuffle: True



  train:
    names:
      - A
      - B
      - C

    # Specify how the data needs to be sliced before feeding to the network.
    # We use a 3D sliding window over the dataset to extract patches, which
    # are then fed to the network as batches.
    slicing_config:
      # Sliding window size
      window_size:
        A:
          - *z_size
          - *xy_size_precrop
          - *xy_size_precrop
        B:
          - *z_size
          - *xy_size_precrop
          - *xy_size_precrop
        C:
          - *z_size
          - *xy_size_precrop
          - *xy_size_precrop
      # Sliding window stride
      stride:
        A: *stride
        B: *stride
        C: *stride
      # Data slice to iterate over.
      data_slice:
        A: ':, :, :'
        B: ':, :, :'
        C: ':70, :, :'

    # Specify paths to volumes
    volume_config:
      # Raw data
      raw:
        path:
          A: '$HCI_HOME/datasets/CREMI/padded_data/sample_A.h5'
          B: '$HCI_HOME/datasets/CREMI/padded_data/sample_B.h5'
          C: '$HCI_HOME/datasets/CREMI/padded_data/sample_C.h5'
        path_in_file: 'volumes/raw'
        dtype: float32
        sigma: 0.025
#        padding_mode: "reflect"
#        padding: &dataset_padding [[0,0], [50,50], [50,50]]

      # Segmentation
      segmentation:
        path:
          A: '$HCI_HOME/datasets/CREMI/padded_data/sample_A.h5'
          B: '$HCI_HOME/datasets/CREMI/padded_data/sample_B.h5'
          C: '$HCI_HOME/datasets/CREMI/padded_data/sample_C.h5'
        path_in_file: 'volumes/labels/neuron_ids'
        dtype: int32
#        padding_mode: "constant"
#        padding: *dataset_padding



  val:
    names:
      - C

    slicing_config:
      window_size:
        C:
          - *z_size
          - *xy_size_precrop
          - *xy_size_precrop
      stride:
        C: *stride
      data_slice:
        C: '70:, :, :' # 75

    volume_config:
      raw:
        path:
          C: '$HCI_HOME/datasets/CREMI/padded_data/sample_C.h5'
        path_in_file: 'volumes/raw'
        dtype: float32
#        sigma: 0.025
#        padding_mode: "reflect"
#        padding: *dataset_padding
      segmentation:
        path:
          C: '$HCI_HOME/datasets/CREMI/padded_data/sample_C.h5'
        path_in_file: 'volumes/labels/neuron_ids'
        dtype: int32
#        padding_mode: "constant"
#        padding: *dataset_padding


model:
  model_class: confnets.models.MultiScaleInputUNet
  model_kwargs:
    in_channels: 1
    out_channels: 20
    final_activation: "Sigmoid"
    depth: 3
    upsampling_mode: 'nearest'
    res_blocks_specs: [[True], [True], [True], [True]]
    res_blocks_specs_decoder: [[True], [True], [True], [True]]
    encoder_fmaps: [32, 64, 128, 256]
    decoder_fmaps: [48, 64, 128, 256]
#    return_input: True
    number_multiscale_inputs: 1
    scale_factor: [1, 2, 2]

#    decoder_crops: # Crops AFTER the res_blocks at each level (at zero, we crop at the end)
#      0: ":, 8:-8, 8:-8"
#      1: ":, 4:-4, 4:-4"
#      2: ":, 2:-2, 2:-2"



trainer:
  max_epochs: 999 # basically infinite
  num_targets: 1

  criterion:
    loss_name: "inferno.extensions.criteria.set_similarity_measures.SorensenDiceLoss"
    kwargs: {}
    transforms:
      - neurofire.criteria.loss_transforms.RemoveSegmentationFromTarget: {}
      - segmfriends.transform.volume.ApplyAndRemoveMask: {first_invert_target: True}

  optimizer:
    Adam:
      lr: 0.0001
      weight_decay: 0.0005
      amsgrad: True
#      betas: [0.9, 0.999]

  intervals:
    save_every: [1000, 'iterations']
    validate_every:
      frequency : [100, 'iterations']
      for_num_iterations: 5

  tensorboard:
    log_scalars_every: [1, 'iterations']
    log_images_every: [500, 'iterations']
    log_histograms_every: 'never'
    send_image_at_batch_indices: [0]
    send_image_at_channel_indices: [0]

  callbacks:
#    gradients:
#      LogOutputGradients:
#        frequency: 1

    essentials:
      SaveAtBestValidationScore:
        smoothness: 0
        verbose: True
      GarbageCollection: {}
#      GradientClip:
#        clip_value: 1e-3

    scheduling:
      AutoLR:
        monitor: 'validation_loss'
        factor: 0.99
        patience: '100 iterations'
        monitor_while: 'validating'
        monitor_momentum: 0.75
#        cooldown_duration: '50000 iterations'
        consider_improvement_with_respect_to: 'previous'
        verbose: True



firelight:
  affinities:
    ImageGridVisualizer:

      input_mapping:
        global: [B: 0, D: "3:9"] # the mapping specified in 'global' is applied to all keys

      pad_width: 1  # width of the border between images in pixels
      pad_value: .2  # intensity of the border pixels
      upsampling_factor: 1  # the whole grid is upsampled by this factor

      row_specs: ['H', 'S', 'B', 'C', 'V']
      column_specs: ['W', 'D']

      visualizers:

        - SegmentationVisualizer:
            input: ['target', index: 0, C: 0]
            background_label: 0
        - IdentityVisualizer:
            input: ['inputs', index: 0]
            cmap: gray
        - IdentityVisualizer:
            input: ['prediction', index: 0, C: ":10"]
            cmap: gray
            value_range: [0,1]
        - IdentityVisualizer:
            input: ['target', index: 0, C: "1:11"]
            cmap: gray_r
            value_range: [0,1]

